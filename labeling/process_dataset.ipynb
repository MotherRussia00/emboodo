{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 254,
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "# os.listdir('../dataset_sheets/pre_pipeline')\n",
    "df = pd.read_csv('../dataset_sheets/pre_pipeline/labeling/data-hunters-handmarked.csv',on_bad_lines='skip',sep=';')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "source": [
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, BertPreTrainedModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification\n",
    "from torch.nn import BCEWithLogitsLoss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "def add_deleted_name(df): #returns df with new column where data vendor name is deleted from dataset name\n",
    "    # df = pd.read_csv(path).fillna('')\n",
    "    df['replaced'] = ''\n",
    "    for i,row in df.iterrows():\n",
    "        row['Title'] = row['Title'].replace(row['Data Provider'],'').strip('\\'s').strip(': ').strip(' - ').strip(' | ')\n",
    "    return df\n",
    "\n",
    "\n",
    "df = df.fillna('')\n",
    "df = df[df['Data Category']!='']\n",
    "print(df.columns)\n",
    "# df = df [['Title', 'Data Category']]\n",
    "df[['Title', 'Data Category']]\n",
    "df = add_deleted_name(df)\n",
    "df[['Title', 'Data Provider']]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['ID', 'Title', 'Content', 'Excerpt', 'Data Provider', 'Use Case',\n",
      "       'Data Category', 'Related Data sets', 'replaced'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Data Provider</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KYC Solution</td>\n",
       "      <td>Datarama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wealth Finder</td>\n",
       "      <td>Datarama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Risk Consulting</td>\n",
       "      <td>Datarama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>STREAM</td>\n",
       "      <td>DataSift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Opt-In Email List</td>\n",
       "      <td>Datastream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37079</th>\n",
       "      <td>Greenspace and sudden unexpected death</td>\n",
       "      <td>U.S. EPA Office of Research and Development (ORD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37080</th>\n",
       "      <td>Arden-Literature Review of Constructed Wetland...</td>\n",
       "      <td>U.S. EPA Office of Research and Development (ORD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37085</th>\n",
       "      <td>Greenspaces and biomarkers of allostatic load</td>\n",
       "      <td>U.S. EPA Office of Research and Development (ORD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37086</th>\n",
       "      <td>LCA of fuels and stoves in India, China, Ghana...</td>\n",
       "      <td>U.S. EPA Office of Research and Development (ORD)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37088</th>\n",
       "      <td>Census and Facility Emissions Dataset</td>\n",
       "      <td>U.S. EPA Office of Research and Development (ORD)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33723 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0                                           KYC Solution   \n",
       "1                                          Wealth Finder   \n",
       "2                                        Risk Consulting   \n",
       "3                                                 STREAM   \n",
       "4                                      Opt-In Email List   \n",
       "...                                                  ...   \n",
       "37079             Greenspace and sudden unexpected death   \n",
       "37080  Arden-Literature Review of Constructed Wetland...   \n",
       "37085      Greenspaces and biomarkers of allostatic load   \n",
       "37086  LCA of fuels and stoves in India, China, Ghana...   \n",
       "37088              Census and Facility Emissions Dataset   \n",
       "\n",
       "                                           Data Provider  \n",
       "0                                               Datarama  \n",
       "1                                               Datarama  \n",
       "2                                               Datarama  \n",
       "3                                               DataSift  \n",
       "4                                             Datastream  \n",
       "...                                                  ...  \n",
       "37079  U.S. EPA Office of Research and Development (ORD)  \n",
       "37080  U.S. EPA Office of Research and Development (ORD)  \n",
       "37085  U.S. EPA Office of Research and Development (ORD)  \n",
       "37086  U.S. EPA Office of Research and Development (ORD)  \n",
       "37088  U.S. EPA Office of Research and Development (ORD)  \n",
       "\n",
       "[33723 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 268
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "source": [
    "allcats = {}\n",
    "for i,row in df.iterrows():\n",
    "    temp = row['Data Category'].split('|')\n",
    "    # print(temp)\n",
    "    for q in temp:\n",
    "        if q in allcats:\n",
    "            allcats[q] += 1\n",
    "        else:\n",
    "            allcats[q] = 1\n",
    "\n",
    "ac = dict(sorted(allcats.items(), key=lambda item: item[1], reverse=True))\n",
    "print(ac)\n",
    "print(len(ac))\n",
    "# ac = dict(sorted(allcats.items(), key=lambda item: item[1]))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Environmental Data': 21255, 'Geospatial Data': 21032, 'Map Data': 7257, 'Demographic Data': 4937, 'Census and Population Data': 3865, 'Economic Data': 3532, 'Construction Industry Data': 1307, 'Soil Health Data': 1196, 'Employment Data': 1071, 'Satellite Data': 787, 'B2B Intent Data': 638, 'Company Data': 582, 'Stock & Market Data': 561, 'Firmographic Data': 404, 'Tourism Data': 382, 'Traffic and Routing Data': 364, 'Energy Data': 326, 'Education Industry Data': 301, 'Real Estate Data': 298, 'Legal Case Data': 289, 'Legal and IP Data': 276, 'Trade Data': 275, 'Public Health Data': 257, 'Business Credit Rating Data': 236, 'Web Data': 232, 'Healthcare Data': 229, 'Marketing Attribution Data': 215, 'Individual Data': 202, 'Agriculture Data': 201, 'B2B Data': 195, 'Business Registry Data': 190, 'Retail & Commerce Data': 148, 'Climate and Weather Data': 142, 'Online/Mobile Data': 134, 'Consumer Lifestyle Data': 134, 'SEO & Web Advertising Data': 132, 'Location Data': 129, 'Industry-Specific Data': 102, 'B2B Marketing Fit Data': 102, 'Customer Transaction Data': 101, 'Automotive Industry Data': 97, 'Cross-Device Identity Data': 86, 'Credit Data': 82, 'Identity Data': 63, 'Points of Interest Data': 60, 'Sports & Entertainment Data': 58, 'Political Data': 56, 'Social Data': 56, 'Pesticides Data': 51, 'Maritime Industry Data': 49, 'Real Estate Investment Data': 41, 'C2C E-Commerce Data': 39, 'Social Media Sentiment Data': 39, 'Foreign Exchange Market Data': 38, 'Technographics Data': 37, 'Patent Data': 36, 'Business Review Data': 34, 'App Data': 34, 'Email Data': 34, 'Sports & Entertainment Events Data': 33, 'IP Data': 32, 'Commodity Market Data': 31, 'Product Review Data': 31, 'News Data': 29, 'Hospitality Industry Data': 28, 'Website Traffic Data': 25, 'Aerospace Industry Data': 25, 'Consumer Survey Data': 24, 'Sports Betting Data': 23, 'GPS Data': 22, 'Foot Traffic Data': 22, 'Industry Financial & Employment Data': 21, 'Esports Data': 21, 'TV Streaming Rating Data': 19, 'Biometric Data': 16, 'Clinical Psychology Data': 16, 'Bankruptcy Data': 16, 'Internet of Things (IoT) Data': 15, 'Property Ownership Data': 15, 'Medical Imagery Data': 15, 'Sustainability Data': 15, 'Industry Classification Data': 13, 'Consumer Confidence Data': 13, 'EHR Data': 12, 'Sports Medicine Data': 12, 'Rental Property Data': 12, 'Political Poll Data': 12, 'Natural Disaster Data': 12, 'Website Data': 12, 'Physical Biometrics': 10, 'Patient Data': 10, 'Behavioral Biometrics': 9, 'IoMT: Internet of Medical Things': 8, 'Psychographics Data': 8, 'Clinical Trial Data': 8, 'Space Exploration Data': 8, 'Conservation Data': 7, 'Human Factors Engineering': 7, 'Soccer Data/Football Data': 6, 'Athletic Performance Data': 6, 'Political Corruption Data': 6, 'IIoT: Industrial Internet of Things': 6, 'UFO Data': 6, 'Astronomy Data': 6, 'Space Weather Data': 6, 'Spaceflight Data': 6, 'Nutrition Data': 5, 'Genomic Data': 5, 'Medical Claims Data': 5, 'Endangered Species Data': 5, 'Chronographic Data': 4, 'Agricultural Safety Data': 4, 'Neuroscience Data': 4, 'Baseball Data': 4, 'GMO Data': 3, 'Face Recognition Data': 3, 'Golf Data': 3, 'Wearable Technology Data': 3, 'Music Data': 3, 'Judicial Data': 3, 'Animal and Plant Species Classification Data': 3, 'Animal Tagging/Wildlife Tracking Data': 3, 'Terrorism Data': 3, 'Trademark Data': 3, 'Ice Hockey Data': 2, 'Basketball Data': 2, 'American Football Data': 2, 'Ocean Sensor Data': 2, 'Organizational Psychology Data': 2, 'Targeted Marketing': 2, 'Psychology Data': 1, 'Paralympic Data': 1, 'Olympics Data': 1, 'Transparent Pro data dashboard allows its users to Track Market Rates & Occupancy, Analyze Demand Events, Monitor Competitors & Supply, and Analyse Ratings & Reviews.': 1, 'Personal Data': 1, 'Persona Detection/Segmentation': 1, 'Inventory Stock Prediction': 1, 'Online and Social Media Performance Tracking': 1, 'Hedge Fund Management': 1, 'Brand Awareness': 1, 'Algo-Trading': 1, 'People and Employment Data': 1, 'Tennis Data': 1, 'Lead Scoring': 1, 'Marketing Localization Strategy': 1, 'AthleteMonitoring Data primarily consists of internal data from user biometrics and questionnaire feedback. Users can also import data from other sources, such as EMR documents.': 1, 'V2X Technology': 1, 'Autonomous Navigation System': 1, 'Fleet Management': 1}\n",
      "149\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "acc = {k:v for k,v in ac.items() if v > 50}\n",
    "label_list = list(acc.keys())\n",
    "print(len(acc))\n",
    "print(label_list)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49\n",
      "['Environmental Data', 'Geospatial Data', 'Map Data', 'Demographic Data', 'Census and Population Data', 'Economic Data', 'Construction Industry Data', 'Soil Health Data', 'Employment Data', 'Satellite Data', 'B2B Intent Data', 'Company Data', 'Stock & Market Data', 'Firmographic Data', 'Tourism Data', 'Traffic and Routing Data', 'Energy Data', 'Education Industry Data', 'Real Estate Data', 'Legal Case Data', 'Legal and IP Data', 'Trade Data', 'Public Health Data', 'Business Credit Rating Data', 'Web Data', 'Healthcare Data', 'Marketing Attribution Data', 'Individual Data', 'Agriculture Data', 'B2B Data', 'Business Registry Data', 'Retail & Commerce Data', 'Climate and Weather Data', 'Online/Mobile Data', 'Consumer Lifestyle Data', 'SEO & Web Advertising Data', 'Location Data', 'Industry-Specific Data', 'B2B Marketing Fit Data', 'Customer Transaction Data', 'Automotive Industry Data', 'Cross-Device Identity Data', 'Credit Data', 'Identity Data', 'Points of Interest Data', 'Sports & Entertainment Data', 'Political Data', 'Social Data', 'Pesticides Data']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "def vectorize(s, ac):\n",
    "    vector = [1 if i in s.split('|') else 0 for i in ac]\n",
    "    return vector\n",
    "\n",
    "    \n",
    "df['List'] = df['Title'].apply(vectorize, ac=acc)\n",
    "print(df.columns)\n",
    "print(df['List'][:10])\n",
    "df_new = df[['Title', 'List']]"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Index(['ID', 'Title', 'Content', 'Excerpt', 'Data Provider', 'Use Case',\n",
      "       'Data Category', 'Related Data sets', 'replaced', 'List'],\n",
      "      dtype='object')\n",
      "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "2    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "5    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "6    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "7    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "8    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "9    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
      "Name: List, dtype: object\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 100\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 4\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-05\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "print(cuda.is_available())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.comment_text = dataframe.Title\n",
    "        self.targets = self.data.List\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comment_text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment_text = str(self.comment_text[index])\n",
    "        comment_text = \" \".join(comment_text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            comment_text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "train_size = 0.9\n",
    "train_dataset=df_new.sample(frac=train_size,random_state=200)\n",
    "test_dataset=df_new.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(df_new.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FULL Dataset: (33723, 2)\n",
      "TRAIN Dataset: (30351, 2)\n",
      "TEST Dataset: (3372, 2)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, len(acc))\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (l1): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (l2): Dropout(p=0.3, inplace=False)\n",
       "  (l3): Linear(in_features=768, out_features=49, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 301
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "source": [
    "def train(epoch):\n",
    "    print(len(training_loader))\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "        print(_)\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3794\n",
      "0\n",
      "Epoch: 0, Loss:  0.6810389757156372\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/1t/10n3n3bd6yn3sw0n1ct2110m0000gp/T/ipykernel_16445/361968077.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/1t/10n3n3bd6yn3sw0n1ct2110m0000gp/T/ipykernel_16445/914265221.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join('../labeling/models', \"finetuned_pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "model_state_dict = torch.load('../labeling/models/finetuned_pytorch_model.bin')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "INFERENCE"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "class BertForMultiLabelSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, size=-1):\n",
    "        filename = 'train.csv'\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, filename)))\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"train\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"train\")\n",
    "        \n",
    "    def get_dev_examples(self, data_dir, size=-1):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        filename = 'val.csv'\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"dev\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"dev\")\n",
    "\n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, data_file_name))\n",
    "#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "        if size == -1:\n",
    "            return self._create_examples(data_df, \"test\")\n",
    "        else:\n",
    "            return self._create_examples(data_df.sample(size), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.labels == None:\n",
    "            self.labels = list(pd.read_csv(os.path.join(self.data_dir, \"classes.txt\"),header=None)[0].values)\n",
    "        return self.labels\n",
    "\n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, row) in enumerate(df.values):\n",
    "            guid = row[0]\n",
    "            text_a = row[1]\n",
    "            if labels_available:\n",
    "                labels = row[2:]\n",
    "            else:\n",
    "                labels = []\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "        return examples"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    # \"full_data_dir\": DATA_PATH,\n",
    "    # \"data_dir\": PATH,\n",
    "    \"task_name\": \"toxic_multilabel\",\n",
    "    \"no_cuda\": False,\n",
    "    # \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    # \"output_dir\": CLAS_DATA_PATH/'output',\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 32,\n",
    "    \"learning_rate\": 3e-5,\n",
    "    \"num_train_epochs\": 4.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "def predict(model, path, test_filename='test_gov_lt.csv'):\n",
    "    predict_processor = MultiLabelTextProcessor(path)\n",
    "    test_examples = predict_processor.get_test_examples(path, test_filename, size=-1)\n",
    "    \n",
    "    # Hold input data for returning it \n",
    "    input_data = [{ 'id': input_example.guid, 'Title': input_example.text_a } for input_example in test_examples]\n",
    "    print('IMPUT DATA', input_data)\n",
    "\n",
    "    test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    \n",
    "    logger.info(\"***** Running prediction *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "        input_ids, input_mask, segment_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            logits = logits.sigmoid()\n",
    "\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    return pd.merge(pd.DataFrame(input_data), pd.DataFrame(all_logits, columns=label_list), left_index=True, right_index=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = BertForMultiLabelSequenceClassification.from_pretrained('bert-base-uncased', num_labels = len(acc), state_dict=model_state_dict)\n",
    "model.to(device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_PATH = '../dataset_sheets/pre_pipeline/labeling/'\n",
    "result = predict(model, DATA_PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "source": [
    "dfff = pd.read_csv(os.path.join(DATA_PATH, 'test_gov_lt.csv'))#.create_index( inplace=True)\n",
    "a = dfff.head\n",
    "a\n",
    "# dfff.columns\n",
    "len(dfff)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "source": [
    "result.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(255, 43)"
      ]
     },
     "metadata": {},
     "execution_count": 136
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "source": [
    "cols = acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "source": [
    "result.columns\n",
    "result.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Demographic Data</th>\n",
       "      <th>Census and Population Data</th>\n",
       "      <th>Economic Data</th>\n",
       "      <th>Construction Industry Data</th>\n",
       "      <th>Employment Data</th>\n",
       "      <th>B2B Intent Data</th>\n",
       "      <th>Company Data</th>\n",
       "      <th>Stock &amp; Market Data</th>\n",
       "      <th>...</th>\n",
       "      <th>Legal and IP Data</th>\n",
       "      <th>Consumer Lifestyle Data</th>\n",
       "      <th>Customer Transaction Data</th>\n",
       "      <th>Industry-Specific Data</th>\n",
       "      <th>B2B Marketing Fit Data</th>\n",
       "      <th>Cross-Device Identity Data</th>\n",
       "      <th>Automotive Industry Data</th>\n",
       "      <th>Credit Data</th>\n",
       "      <th>Points of Interest Data</th>\n",
       "      <th>Social Data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>List of companies licensed to produce beer</td>\n",
       "      <td>0.616299</td>\n",
       "      <td>0.507772</td>\n",
       "      <td>0.557883</td>\n",
       "      <td>0.476998</td>\n",
       "      <td>0.409134</td>\n",
       "      <td>0.412928</td>\n",
       "      <td>0.412344</td>\n",
       "      <td>0.451411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.430333</td>\n",
       "      <td>0.517916</td>\n",
       "      <td>0.596153</td>\n",
       "      <td>0.499197</td>\n",
       "      <td>0.468454</td>\n",
       "      <td>0.644364</td>\n",
       "      <td>0.446601</td>\n",
       "      <td>0.537777</td>\n",
       "      <td>0.482502</td>\n",
       "      <td>0.526952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>List of companies licensed to produce alcoholi...</td>\n",
       "      <td>0.618152</td>\n",
       "      <td>0.499477</td>\n",
       "      <td>0.560525</td>\n",
       "      <td>0.486208</td>\n",
       "      <td>0.426581</td>\n",
       "      <td>0.428528</td>\n",
       "      <td>0.416714</td>\n",
       "      <td>0.451667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445067</td>\n",
       "      <td>0.505346</td>\n",
       "      <td>0.600065</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.477208</td>\n",
       "      <td>0.645918</td>\n",
       "      <td>0.451796</td>\n",
       "      <td>0.547249</td>\n",
       "      <td>0.477928</td>\n",
       "      <td>0.540403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List of companies holding licenses for the pro...</td>\n",
       "      <td>0.610954</td>\n",
       "      <td>0.480690</td>\n",
       "      <td>0.573754</td>\n",
       "      <td>0.483164</td>\n",
       "      <td>0.431224</td>\n",
       "      <td>0.426026</td>\n",
       "      <td>0.427970</td>\n",
       "      <td>0.477310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.437029</td>\n",
       "      <td>0.512694</td>\n",
       "      <td>0.578926</td>\n",
       "      <td>0.516095</td>\n",
       "      <td>0.448400</td>\n",
       "      <td>0.643474</td>\n",
       "      <td>0.457748</td>\n",
       "      <td>0.552868</td>\n",
       "      <td>0.447352</td>\n",
       "      <td>0.510053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Other classifications of JAR attributes</td>\n",
       "      <td>0.596935</td>\n",
       "      <td>0.495317</td>\n",
       "      <td>0.552336</td>\n",
       "      <td>0.454822</td>\n",
       "      <td>0.411303</td>\n",
       "      <td>0.418416</td>\n",
       "      <td>0.426577</td>\n",
       "      <td>0.497369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447986</td>\n",
       "      <td>0.530654</td>\n",
       "      <td>0.579579</td>\n",
       "      <td>0.510689</td>\n",
       "      <td>0.471135</td>\n",
       "      <td>0.627860</td>\n",
       "      <td>0.450128</td>\n",
       "      <td>0.549728</td>\n",
       "      <td>0.481182</td>\n",
       "      <td>0.500733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Open design data of the Republic of Lithuania</td>\n",
       "      <td>0.592917</td>\n",
       "      <td>0.480318</td>\n",
       "      <td>0.577382</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.404420</td>\n",
       "      <td>0.396464</td>\n",
       "      <td>0.423490</td>\n",
       "      <td>0.459900</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446574</td>\n",
       "      <td>0.507616</td>\n",
       "      <td>0.563264</td>\n",
       "      <td>0.512501</td>\n",
       "      <td>0.446403</td>\n",
       "      <td>0.643013</td>\n",
       "      <td>0.468283</td>\n",
       "      <td>0.566291</td>\n",
       "      <td>0.481887</td>\n",
       "      <td>0.499442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              Title  Demographic Data  \\\n",
       "0   0         List of companies licensed to produce beer          0.616299   \n",
       "1   1  List of companies licensed to produce alcoholi...          0.618152   \n",
       "2   2  List of companies holding licenses for the pro...          0.610954   \n",
       "3   3            Other classifications of JAR attributes          0.596935   \n",
       "4   4      Open design data of the Republic of Lithuania          0.592917   \n",
       "\n",
       "   Census and Population Data  Economic Data  Construction Industry Data  \\\n",
       "0                    0.507772       0.557883                    0.476998   \n",
       "1                    0.499477       0.560525                    0.486208   \n",
       "2                    0.480690       0.573754                    0.483164   \n",
       "3                    0.495317       0.552336                    0.454822   \n",
       "4                    0.480318       0.577382                    0.475000   \n",
       "\n",
       "   Employment Data  B2B Intent Data  Company Data  Stock & Market Data  ...  \\\n",
       "0         0.409134         0.412928      0.412344             0.451411  ...   \n",
       "1         0.426581         0.428528      0.416714             0.451667  ...   \n",
       "2         0.431224         0.426026      0.427970             0.477310  ...   \n",
       "3         0.411303         0.418416      0.426577             0.497369  ...   \n",
       "4         0.404420         0.396464      0.423490             0.459900  ...   \n",
       "\n",
       "   Legal and IP Data  Consumer Lifestyle Data  Customer Transaction Data  \\\n",
       "0           0.430333                 0.517916                   0.596153   \n",
       "1           0.445067                 0.505346                   0.600065   \n",
       "2           0.437029                 0.512694                   0.578926   \n",
       "3           0.447986                 0.530654                   0.579579   \n",
       "4           0.446574                 0.507616                   0.563264   \n",
       "\n",
       "   Industry-Specific Data  B2B Marketing Fit Data  Cross-Device Identity Data  \\\n",
       "0                0.499197                0.468454                    0.644364   \n",
       "1                0.500034                0.477208                    0.645918   \n",
       "2                0.516095                0.448400                    0.643474   \n",
       "3                0.510689                0.471135                    0.627860   \n",
       "4                0.512501                0.446403                    0.643013   \n",
       "\n",
       "   Automotive Industry Data  Credit Data  Points of Interest Data  Social Data  \n",
       "0                  0.446601     0.537777                 0.482502     0.526952  \n",
       "1                  0.451796     0.547249                 0.477928     0.540403  \n",
       "2                  0.457748     0.552868                 0.447352     0.510053  \n",
       "3                  0.450128     0.549728                 0.481182     0.500733  \n",
       "4                  0.468283     0.566291                 0.481887     0.499442  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 141
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "POSTPROCESSING"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "result_0 = result.iloc[:,:2]\n",
    "result_0"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>List of companies licensed to produce beer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>List of companies licensed to produce alcoholi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>List of companies holding licenses for the pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Other classifications of JAR attributes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Open design data of the Republic of Lithuania</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>250</td>\n",
       "      <td>Data from several projects</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>Road weather data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>252</td>\n",
       "      <td>Road network data of national and local signif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>253</td>\n",
       "      <td>Financial data on road asset values and deprec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>254</td>\n",
       "      <td>Data on toll road sections</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>255 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              Title\n",
       "0      0         List of companies licensed to produce beer\n",
       "1      1  List of companies licensed to produce alcoholi...\n",
       "2      2  List of companies holding licenses for the pro...\n",
       "3      3            Other classifications of JAR attributes\n",
       "4      4      Open design data of the Republic of Lithuania\n",
       "..   ...                                                ...\n",
       "250  250                         Data from several projects\n",
       "251  251                                  Road weather data\n",
       "252  252  Road network data of national and local signif...\n",
       "253  253  Financial data on road asset values and deprec...\n",
       "254  254                         Data on toll road sections\n",
       "\n",
       "[255 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 217
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "result_1 = result.iloc[:,2:]\n",
    "for i in result_1.columns:\n",
    "    result_1[i] = np.where((result_1[i]<=0.6), 0, result_1[i])\n",
    "    result_1[i] = np.where((result_1[i]>0.6), i, result_1[i])\n",
    "\n",
    "result_1 = result_1.replace('0.0','')\n",
    "result_1['data_categories'] = result_1.values.tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "\n",
    "def delete_empty(ll):\n",
    "    l = []\n",
    "    for i in ll:\n",
    "        if i:\n",
    "            l.append(i)\n",
    "    return l\n",
    "\n",
    "result_1['data_categories'] = result_1['data_categories'].apply(delete_empty)\n",
    "result_1['data_categories'] = result_1['data_categories'].apply(lambda x: ','.join(x))\n",
    "result_2 = result_1[['data_categories']]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "source": [
    "final = pd.merge(result_0, result_2,left_index=True, right_index=True)\n",
    "final\n",
    "final.to_csv('../results/pre_pipeline/data.gov.lt-datacat-bert.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}