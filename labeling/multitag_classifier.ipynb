{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "from transformers import BertModel,BertTokenizer,AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import torch\n",
    "from torch import nn ,cuda\n",
    "from torch.utils.data import DataLoader,Dataset,RandomSampler, SequentialSampler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dataset_sheets\\Datahunters complete upload 23-01-22.csv').fillna('')\n",
    "df['id'] = df.index\n",
    "df = df[['id','post_title','data_categories']]\n",
    "df['post_title'] = df['post_title'].apply(lambda x:x.lower())\n",
    "\n",
    "df['data_categories'] = df['data_categories'].apply(lambda x: x.lower().split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(df['data_categories'])\n",
    "x = list(df['post_title'])\n",
    "y = list(df['data_categories'])\n",
    "# y\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(381, 80)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    " \n",
    "yt = mlb.fit_transform(y)\n",
    "yt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x, yt, test_size=0.1, random_state=RANDOM_SEED,shuffle=True)\n",
    "x_tr,x_val,y_tr,y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=RANDOM_SEED,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(273, 69, 39)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_tr) ,len(x_val), len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "%matplotlib inline\n",
    "questions = x\n",
    "word_cnt = [len(quest.split()) for quest in questions]\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=[8,5])\n",
    "plt.hist(word_cnt, bins = 10)\n",
    "plt.xlabel('Word Count/phrase')\n",
    "plt.ylabel('# of Occurences')\n",
    "plt.title(\"Frequency of Word Counts/sentence\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTagDataset (Dataset):\n",
    "    def __init__(self,quest,tags, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = quest\n",
    "        self.labels = tags\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "    \n",
    "    def __getitem__(self, item_idx):\n",
    "        text = self.text[item_idx]\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True, # Add [CLS] [SEP]\n",
    "            max_length= self.max_len,\n",
    "            padding = 'max_length',\n",
    "            return_token_type_ids= False,\n",
    "            return_attention_mask= True, # Differentiates padded vs normal token\n",
    "            truncation=True, # Truncate data beyond max length\n",
    "            return_tensors = 'pt' # PyTorch Tensor format\n",
    "          )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].flatten()\n",
    "        attn_mask = inputs['attention_mask'].flatten()\n",
    "        #token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids ,\n",
    "            'attention_mask': attn_mask,\n",
    "            'label': torch.tensor(self.labels[item_idx], dtype=torch.float)\n",
    "            \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTagDataModule (pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self,x_tr,y_tr,x_val,y_val,x_test,y_test,tokenizer,batch_size=16,max_token_len=200):\n",
    "        super().__init__()\n",
    "        self.tr_text = x_tr\n",
    "        self.tr_label = y_tr\n",
    "        self.val_text = x_val\n",
    "        self.val_label = y_val\n",
    "        self.test_text = x_test\n",
    "        self.test_label = y_test\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_token_len = max_token_len\n",
    "\n",
    "    def setup(self):\n",
    "        self.train_dataset = QTagDataset(quest=self.tr_text, tags=self.tr_label, tokenizer=self.tokenizer,max_len = self.max_token_len)\n",
    "        self.val_dataset  = QTagDataset(quest=self.val_text,tags=self.val_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\n",
    "        self.test_dataset  = QTagDataset(quest=self.test_text,tags=self.test_label,tokenizer=self.tokenizer,max_len = self.max_token_len)\n",
    "        \n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader (self.train_dataset,batch_size = self.batch_size,shuffle = True , num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader (self.val_dataset,batch_size= 16)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader (self.test_dataset,batch_size= 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_NAME = \"bert-base-cased\" # we will use the BERT base model(the smaller one)\n",
    "Bert_tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for question in questions:\n",
    "\n",
    "    # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
    "    input_ids = Bert_tokenizer.encode(question, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters that will be use for training\n",
    "N_EPOCHS = 12\n",
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 30\n",
    "LR = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "QTdata_module = QTagDataModule(x_tr,y_tr,x_val,y_val,x_test,y_test,Bert_tokenizer,BATCH_SIZE,MAX_LEN)\n",
    "QTdata_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTagClassifier(pl.LightningModule):\n",
    "    # Set up the classifier\n",
    "    def __init__(self, n_classes=80, steps_per_epoch=None, n_epochs=3, lr=2e-5 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_NAME, return_dict=True)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size,n_classes) # outputs = number of labels\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "        self.n_epochs = n_epochs\n",
    "        self.lr = lr\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(self,input_ids, attn_mask):\n",
    "        output = self.bert(input_ids = input_ids ,attention_mask = attn_mask)\n",
    "        output = self.classifier(output.pooler_output)\n",
    "                \n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('train_loss',loss , prog_bar=True,logger=True)\n",
    "        \n",
    "        return {\"loss\" :loss, \"predictions\":outputs, \"labels\": labels }\n",
    "\n",
    "\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('val_loss',loss , prog_bar=True,logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        outputs = self(input_ids,attention_mask)\n",
    "        loss = self.criterion(outputs,labels)\n",
    "        self.log('test_loss',loss , prog_bar=True,logger=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters() , lr=self.lr)\n",
    "        warmup_steps = self.steps_per_epoch//3\n",
    "        total_steps = self.steps_per_epoch * self.n_epochs - warmup_steps\n",
    "\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer,warmup_steps,total_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(x_tr)//BATCH_SIZE\n",
    "model = QTagClassifier(n_classes=80, steps_per_epoch=steps_per_epoch,n_epochs=N_EPOCHS,lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Pytorch Lightning callback for Model checkpointing\n",
    "\n",
    "# saves a file like: input/QTag-epoch=02-val_loss=0.32.ckpt\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',# monitored quantity\n",
    "    filename='QTag-{epoch:02d}-{val_loss:.2f}',\n",
    "    save_top_k=3, #  save the top 3 models\n",
    "    mode='min', # mode of the monitored quantity  for optimization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=10)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs = N_EPOCHS , gpus = 0, callbacks=[checkpoint_callback],progress_bar_refresh_rate = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\core\\datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert       | BertModel         | 108 M \n",
      "1 | classifier | Linear            | 61.5 K\n",
      "2 | criterion  | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "433.487   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:132: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\leozi\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:432: UserWarning: The number of training samples (18) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/23 [00:00<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "trainer.fit(model, QTdata_module)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a948559c7a4216505b87a95f1b71f683a4ab06718d8a6d1e74cfa6ec61a60d6f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
